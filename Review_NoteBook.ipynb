{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This notebook contains the final optimized M5 forecasting pipeline used for the Retail Sales Analytics Dashboard\n"
     ]
    }
   ],
   "source": [
    "## Project Note\n",
    "print(\"This notebook contains the final optimized M5 forecasting pipeline used for the Retail Sales Analytics Dashboard\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-10T15:29:18.840357Z",
     "iopub.status.busy": "2025-12-10T15:29:18.839810Z",
     "iopub.status.idle": "2025-12-10T15:29:18.848298Z",
     "shell.execute_reply": "2025-12-10T15:29:18.847055Z",
     "shell.execute_reply.started": "2025-12-10T15:29:18.840328Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class ProjectConfig:\n",
    "    DATA_PATH = 'D:/M5 Data'   # <-- IMPORTANT: Set your dataset folder path here\n",
    "    TRAIN_END = 1913\n",
    "    FORECAST_HORIZON = 28\n",
    "    RANDOM_STATE = 42\n",
    "    \n",
    "    LGB_PARAMS = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'tweedie',\n",
    "        'tweedie_variance_power': 1.1,\n",
    "        'metric': 'rmse',\n",
    "        'subsample': 0.5,\n",
    "        'subsample_freq': 1,\n",
    "        'learning_rate': 0.03,\n",
    "        'num_leaves': 2047,\n",
    "        'min_data_in_leaf': 4095,\n",
    "        'feature_fraction': 0.5,\n",
    "        'max_bin': 100,\n",
    "        'n_estimators': 1400,\n",
    "        'boost_from_average': False,\n",
    "        'verbose': -1,\n",
    "        'n_jobs': -1\n",
    "    }\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "seed_everything(ProjectConfig.RANDOM_STATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T15:29:23.477093Z",
     "iopub.status.busy": "2025-12-10T15:29:23.476722Z",
     "iopub.status.idle": "2025-12-10T15:29:31.297095Z",
     "shell.execute_reply": "2025-12-10T15:29:31.296057Z",
     "shell.execute_reply.started": "2025-12-10T15:29:23.477069Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sales shape: (30490, 1919)\n",
      "Calendar shape: (1969, 14)\n",
      "Prices shape: (6841121, 4)\n"
     ]
    }
   ],
   "source": [
    "# Load datasets from your D drive\n",
    "\n",
    "sales = pd.read_csv(f\"{ProjectConfig.DATA_PATH}/sales_train_validation.csv\")\n",
    "calendar = pd.read_csv(f\"{ProjectConfig.DATA_PATH}/calendar.csv\")\n",
    "prices = pd.read_csv(f\"{ProjectConfig.DATA_PATH}/sell_prices.csv\")\n",
    "\n",
    "print(\"Sales shape:\", sales.shape)\n",
    "print(\"Calendar shape:\", calendar.shape)\n",
    "print(\"Prices shape:\", prices.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T06:56:22.421718Z",
     "iopub.status.busy": "2025-12-10T06:56:22.421409Z",
     "iopub.status.idle": "2025-12-10T06:56:22.432027Z",
     "shell.execute_reply": "2025-12-10T06:56:22.430998Z",
     "shell.execute_reply.started": "2025-12-10T06:56:22.421697Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def downcast_dtypes(df, verbose=True):\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        \n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            \n",
    "            if str(col_type).startswith('int'):\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose:\n",
    "        print(f'Memory usage dropped to {end_mem:5.2f} Mb ({100 * (start_mem - end_mem) / start_mem:.1f}% reduction)')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T19:32:34.920605Z",
     "iopub.status.busy": "2025-11-23T19:32:34.920026Z",
     "iopub.status.idle": "2025-11-23T19:32:34.942935Z",
     "shell.execute_reply": "2025-11-23T19:32:34.94193Z",
     "shell.execute_reply.started": "2025-11-23T19:32:34.920578Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    print(f\"Reading files from {path}...\")\n",
    "    \n",
    "    calendar = pd.read_csv(f'{path}/calendar.csv')\n",
    "    calendar = downcast_dtypes(calendar)\n",
    "    \n",
    "    prices = pd.read_csv(f'{path}/sell_prices.csv')\n",
    "    prices = downcast_dtypes(prices)\n",
    "    \n",
    "    sales = pd.read_csv(f'{path}/sales_train_validation.csv')\n",
    "    sales = downcast_dtypes(sales)\n",
    "    \n",
    "    return sales, calendar, prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T19:32:34.945204Z",
     "iopub.status.busy": "2025-11-23T19:32:34.944575Z",
     "iopub.status.idle": "2025-11-23T19:32:46.663374Z",
     "shell.execute_reply": "2025-11-23T19:32:46.662424Z",
     "shell.execute_reply.started": "2025-11-23T19:32:34.945179Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading files from D:/M5 Data...\n",
      "Memory usage dropped to  0.12 Mb (41.9% reduction)\n",
      "Memory usage dropped to 130.48 Mb (37.5% reduction)\n",
      "Memory usage dropped to 95.00 Mb (78.7% reduction)\n"
     ]
    }
   ],
   "source": [
    "df_sales, df_calendar, df_prices = read_data(ProjectConfig.DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T19:37:57.136205Z",
     "iopub.status.busy": "2025-11-23T19:37:57.1355Z",
     "iopub.status.idle": "2025-11-23T19:37:57.14288Z",
     "shell.execute_reply": "2025-11-23T19:37:57.142059Z",
     "shell.execute_reply.started": "2025-11-23T19:37:57.136178Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def transform_and_merge(sales, calendar, prices, config):\n",
    "    for day in range(config.FORECAST_HORIZON):\n",
    "        sales[f'd_{config.TRAIN_END + day + 1}'] = np.nan\n",
    "\n",
    "    start_idx = max(1, config.TRAIN_END - 1000) \n",
    "    value_cols = [c for c in sales.columns if c.startswith('d_') and int(c.split('_')[1]) >= start_idx]\n",
    "    \n",
    "    id_cols = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']\n",
    "    \n",
    "    data = pd.melt(sales, id_vars=id_cols, value_vars=value_cols, var_name='d', value_name='sales')\n",
    "    \n",
    "    calendar = calendar.drop(['weekday', 'wday', 'month', 'year'], axis=1)\n",
    "    data = data.merge(calendar, on='d', how='left')\n",
    "    \n",
    "    data = data.merge(prices, on=['store_id', 'item_id', 'wm_yr_wk'], how='left')\n",
    "    \n",
    "    del calendar, prices\n",
    "    gc.collect()\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T19:37:57.274994Z",
     "iopub.status.busy": "2025-11-23T19:37:57.274497Z",
     "iopub.status.idle": "2025-11-23T19:37:57.281424Z",
     "shell.execute_reply": "2025-11-23T19:37:57.280496Z",
     "shell.execute_reply.started": "2025-11-23T19:37:57.274961Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def feature_engineering_basic(df):\n",
    "    df['d_num'] = df['d'].apply(lambda x: x.split('_')[1]).astype(np.int16)\n",
    "    \n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df['day_of_week'] = df['date'].dt.dayofweek.astype(np.int8)\n",
    "    df['month'] = df['date'].dt.month.astype(np.int8)\n",
    "    \n",
    "    df['is_weekend'] = (df['day_of_week'] >= 5).astype(np.int8)\n",
    "    \n",
    "    df['price_momentum'] = df['sell_price'] / df.groupby('id')['sell_price'].transform('mean')\n",
    "    \n",
    "    df = df.drop(['date', 'd'], axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T19:37:57.451236Z",
     "iopub.status.busy": "2025-11-23T19:37:57.450933Z",
     "iopub.status.idle": "2025-11-23T19:37:57.456523Z",
     "shell.execute_reply": "2025-11-23T19:37:57.455551Z",
     "shell.execute_reply.started": "2025-11-23T19:37:57.451216Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def feature_engineering_lags(df):\n",
    "    lags = [28, 35, 42, 49, 56]\n",
    "    for lag in lags:\n",
    "        df[f'lag_{lag}'] = df.groupby('id')['sales'].shift(lag)\n",
    "        \n",
    "    windows = [7, 14, 28, 60]\n",
    "    for win in windows:\n",
    "        df[f'rolling_mean_{win}'] = df.groupby('id')['lag_28'].transform(\n",
    "            lambda x: x.rolling(win).mean())\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T19:44:39.486392Z",
     "iopub.status.busy": "2025-11-23T19:44:39.486026Z",
     "iopub.status.idle": "2025-11-23T19:44:39.493118Z",
     "shell.execute_reply": "2025-11-23T19:44:39.492173Z",
     "shell.execute_reply.started": "2025-11-23T19:44:39.486367Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def encode_categoricals(df):\n",
    "    # Added event_name_2 and event_type_2 to the list\n",
    "    cat_cols = [\n",
    "        'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', \n",
    "        'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2'\n",
    "    ]\n",
    "    \n",
    "    # Fill NaNs for ALL event columns\n",
    "    df['event_name_1'] = df['event_name_1'].fillna('NoEvent')\n",
    "    df['event_type_1'] = df['event_type_1'].fillna('NoEvent')\n",
    "    df['event_name_2'] = df['event_name_2'].fillna('NoEvent')\n",
    "    df['event_type_2'] = df['event_type_2'].fillna('NoEvent')\n",
    "    \n",
    "    encoder = LabelEncoder()\n",
    "    for col in cat_cols:\n",
    "        # We convert to string first to handle any mixed types safely\n",
    "        df[col] = encoder.fit_transform(df[col].astype(str))\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T19:44:41.730409Z",
     "iopub.status.busy": "2025-11-23T19:44:41.7299Z",
     "iopub.status.idle": "2025-11-23T19:49:33.434412Z",
     "shell.execute_reply": "2025-11-23T19:49:33.433149Z",
     "shell.execute_reply.started": "2025-11-23T19:44:41.730379Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def transform_and_merge(sales, calendar, prices, config):\n",
    "    # --------------------------------------------------\n",
    "    # 1. Add future forecast columns\n",
    "    # --------------------------------------------------\n",
    "    for day in range(config.FORECAST_HORIZON):\n",
    "        sales[f'd_{config.TRAIN_END + day + 1}'] = np.nan\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 2. Limit history to last ~1000 days (MEMORY FIX)\n",
    "    # --------------------------------------------------\n",
    "    start_idx = max(1, config.TRAIN_END - 1000)\n",
    "\n",
    "    value_cols = [\n",
    "        c for c in sales.columns\n",
    "        if c.startswith('d_') and int(c.split('_')[1]) >= start_idx\n",
    "    ]\n",
    "\n",
    "    id_cols = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 3. Melt after limiting columns\n",
    "    # --------------------------------------------------\n",
    "    data = pd.melt(\n",
    "        sales,\n",
    "        id_vars=id_cols,\n",
    "        value_vars=value_cols,\n",
    "        var_name='d',\n",
    "        value_name='sales'\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 4. Reduce calendar before merge\n",
    "    # --------------------------------------------------\n",
    "    calendar = calendar[\n",
    "        ['d', 'date', 'wm_yr_wk',\n",
    "         'event_name_1', 'event_type_1',\n",
    "         'event_name_2', 'event_type_2']\n",
    "    ]\n",
    "\n",
    "    data = data.merge(calendar, on='d', how='left')\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 5. Reduce prices before merge (CRITICAL)\n",
    "    # --------------------------------------------------\n",
    "    prices = prices[['store_id', 'item_id', 'wm_yr_wk', 'sell_price']]\n",
    "\n",
    "    data = data.merge(\n",
    "        prices,\n",
    "        on=['store_id', 'item_id', 'wm_yr_wk'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 6. Cleanup\n",
    "    # --------------------------------------------------\n",
    "    del calendar, prices\n",
    "    gc.collect()\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T19:49:33.436449Z",
     "iopub.status.busy": "2025-11-23T19:49:33.436112Z",
     "iopub.status.idle": "2025-11-23T19:49:33.443233Z",
     "shell.execute_reply": "2025-11-23T19:49:33.442062Z",
     "shell.execute_reply.started": "2025-11-23T19:49:33.436413Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def perform_split(df, config):\n",
    "    # --------------------------------------------------\n",
    "    # 1. Create masks (lightweight)\n",
    "    # --------------------------------------------------\n",
    "    train_mask = df['d_num'] <= (config.TRAIN_END - config.FORECAST_HORIZON)\n",
    "    valid_mask = (\n",
    "        (df['d_num'] > (config.TRAIN_END - config.FORECAST_HORIZON)) &\n",
    "        (df['d_num'] <= config.TRAIN_END)\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 2. Define feature columns FIRST (CRITICAL)\n",
    "    # --------------------------------------------------\n",
    "    drop_cols = ['id', 'sales', 'wm_yr_wk', 'd_num']\n",
    "    feat_cols = [c for c in df.columns if c not in drop_cols]\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 3. Select only needed columns BEFORE slicing\n",
    "    # --------------------------------------------------\n",
    "    df_feat = df[feat_cols + ['sales']]\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 4. Split (much smaller memory footprint)\n",
    "    # --------------------------------------------------\n",
    "    X_tr = df_feat.loc[train_mask, feat_cols]\n",
    "    y_tr = df_feat.loc[train_mask, 'sales']\n",
    "\n",
    "    X_val = df_feat.loc[valid_mask, feat_cols]\n",
    "    y_val = df_feat.loc[valid_mask, 'sales']\n",
    "\n",
    "    return X_tr, y_tr, X_val, y_val, feat_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "master_df loaded: (31374210, 30)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "master_df = pd.read_pickle(\"processed_dataset.pkl\")\n",
    "print(\"master_df loaded:\", master_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, y_tr, X_val, y_val, feats = perform_split(master_df, ProjectConfig)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 1236839,
     "sourceId": 18599,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
