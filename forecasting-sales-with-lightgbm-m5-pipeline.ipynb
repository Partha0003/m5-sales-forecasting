{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This notebook contains the final optimized M5 forecasting pipeline used for the Retail Sales Analytics Dashboard\n"
     ]
    }
   ],
   "source": [
    "## Project Note\n",
    "print(\"This notebook contains the final optimized M5 forecasting pipeline used for the Retail Sales Analytics Dashboard\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-10T15:29:18.840357Z",
     "iopub.status.busy": "2025-12-10T15:29:18.839810Z",
     "iopub.status.idle": "2025-12-10T15:29:18.848298Z",
     "shell.execute_reply": "2025-12-10T15:29:18.847055Z",
     "shell.execute_reply.started": "2025-12-10T15:29:18.840328Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class ProjectConfig:\n",
    "    DATA_PATH = 'D:/M5 Data'   # <-- IMPORTANT: Set your dataset folder path here\n",
    "    TRAIN_END = 1913\n",
    "    FORECAST_HORIZON = 28\n",
    "    RANDOM_STATE = 42\n",
    "    \n",
    "    LGB_PARAMS = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'tweedie',\n",
    "        'tweedie_variance_power': 1.1,\n",
    "        'metric': 'rmse',\n",
    "        'subsample': 0.5,\n",
    "        'subsample_freq': 1,\n",
    "        'learning_rate': 0.03,\n",
    "        'num_leaves': 2047,\n",
    "        'min_data_in_leaf': 4095,\n",
    "        'feature_fraction': 0.5,\n",
    "        'max_bin': 100,\n",
    "        'n_estimators': 1400,\n",
    "        'boost_from_average': False,\n",
    "        'verbose': -1,\n",
    "        'n_jobs': -1\n",
    "    }\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "seed_everything(ProjectConfig.RANDOM_STATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T15:29:23.477093Z",
     "iopub.status.busy": "2025-12-10T15:29:23.476722Z",
     "iopub.status.idle": "2025-12-10T15:29:31.297095Z",
     "shell.execute_reply": "2025-12-10T15:29:31.296057Z",
     "shell.execute_reply.started": "2025-12-10T15:29:23.477069Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sales shape: (30490, 1919)\n",
      "Calendar shape: (1969, 14)\n",
      "Prices shape: (6841121, 4)\n"
     ]
    }
   ],
   "source": [
    "# Load datasets from your D drive\n",
    "\n",
    "sales = pd.read_csv(f\"{ProjectConfig.DATA_PATH}/sales_train_validation.csv\")\n",
    "calendar = pd.read_csv(f\"{ProjectConfig.DATA_PATH}/calendar.csv\")\n",
    "prices = pd.read_csv(f\"{ProjectConfig.DATA_PATH}/sell_prices.csv\")\n",
    "\n",
    "print(\"Sales shape:\", sales.shape)\n",
    "print(\"Calendar shape:\", calendar.shape)\n",
    "print(\"Prices shape:\", prices.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T06:56:22.421718Z",
     "iopub.status.busy": "2025-12-10T06:56:22.421409Z",
     "iopub.status.idle": "2025-12-10T06:56:22.432027Z",
     "shell.execute_reply": "2025-12-10T06:56:22.430998Z",
     "shell.execute_reply.started": "2025-12-10T06:56:22.421697Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def downcast_dtypes(df, verbose=True):\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        \n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            \n",
    "            if str(col_type).startswith('int'):\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose:\n",
    "        print(f'Memory usage dropped to {end_mem:5.2f} Mb ({100 * (start_mem - end_mem) / start_mem:.1f}% reduction)')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T19:32:34.920605Z",
     "iopub.status.busy": "2025-11-23T19:32:34.920026Z",
     "iopub.status.idle": "2025-11-23T19:32:34.942935Z",
     "shell.execute_reply": "2025-11-23T19:32:34.94193Z",
     "shell.execute_reply.started": "2025-11-23T19:32:34.920578Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    print(f\"Reading files from {path}...\")\n",
    "    \n",
    "    calendar = pd.read_csv(f'{path}/calendar.csv')\n",
    "    calendar = downcast_dtypes(calendar)\n",
    "    \n",
    "    prices = pd.read_csv(f'{path}/sell_prices.csv')\n",
    "    prices = downcast_dtypes(prices)\n",
    "    \n",
    "    sales = pd.read_csv(f'{path}/sales_train_validation.csv')\n",
    "    sales = downcast_dtypes(sales)\n",
    "    \n",
    "    return sales, calendar, prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T19:32:34.945204Z",
     "iopub.status.busy": "2025-11-23T19:32:34.944575Z",
     "iopub.status.idle": "2025-11-23T19:32:46.663374Z",
     "shell.execute_reply": "2025-11-23T19:32:46.662424Z",
     "shell.execute_reply.started": "2025-11-23T19:32:34.945179Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading files from D:/M5 Data...\n",
      "Memory usage dropped to  0.12 Mb (41.9% reduction)\n",
      "Memory usage dropped to 130.48 Mb (37.5% reduction)\n",
      "Memory usage dropped to 95.00 Mb (78.7% reduction)\n"
     ]
    }
   ],
   "source": [
    "df_sales, df_calendar, df_prices = read_data(ProjectConfig.DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T19:37:57.136205Z",
     "iopub.status.busy": "2025-11-23T19:37:57.1355Z",
     "iopub.status.idle": "2025-11-23T19:37:57.14288Z",
     "shell.execute_reply": "2025-11-23T19:37:57.142059Z",
     "shell.execute_reply.started": "2025-11-23T19:37:57.136178Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def transform_and_merge(sales, calendar, prices, config):\n",
    "    for day in range(config.FORECAST_HORIZON):\n",
    "        sales[f'd_{config.TRAIN_END + day + 1}'] = np.nan\n",
    "\n",
    "    start_idx = max(1, config.TRAIN_END - 1000) \n",
    "    value_cols = [c for c in sales.columns if c.startswith('d_') and int(c.split('_')[1]) >= start_idx]\n",
    "    \n",
    "    id_cols = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']\n",
    "    \n",
    "    data = pd.melt(sales, id_vars=id_cols, value_vars=value_cols, var_name='d', value_name='sales')\n",
    "    \n",
    "    calendar = calendar.drop(['weekday', 'wday', 'month', 'year'], axis=1)\n",
    "    data = data.merge(calendar, on='d', how='left')\n",
    "    \n",
    "    data = data.merge(prices, on=['store_id', 'item_id', 'wm_yr_wk'], how='left')\n",
    "    \n",
    "    del calendar, prices\n",
    "    gc.collect()\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T19:37:57.274994Z",
     "iopub.status.busy": "2025-11-23T19:37:57.274497Z",
     "iopub.status.idle": "2025-11-23T19:37:57.281424Z",
     "shell.execute_reply": "2025-11-23T19:37:57.280496Z",
     "shell.execute_reply.started": "2025-11-23T19:37:57.274961Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def feature_engineering_basic(df):\n",
    "    df['d_num'] = df['d'].apply(lambda x: x.split('_')[1]).astype(np.int16)\n",
    "    \n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df['day_of_week'] = df['date'].dt.dayofweek.astype(np.int8)\n",
    "    df['month'] = df['date'].dt.month.astype(np.int8)\n",
    "    \n",
    "    df['is_weekend'] = (df['day_of_week'] >= 5).astype(np.int8)\n",
    "    \n",
    "    df['price_momentum'] = df['sell_price'] / df.groupby('id')['sell_price'].transform('mean')\n",
    "    \n",
    "    df = df.drop(['date', 'd'], axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T19:37:57.451236Z",
     "iopub.status.busy": "2025-11-23T19:37:57.450933Z",
     "iopub.status.idle": "2025-11-23T19:37:57.456523Z",
     "shell.execute_reply": "2025-11-23T19:37:57.455551Z",
     "shell.execute_reply.started": "2025-11-23T19:37:57.451216Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def feature_engineering_lags(df):\n",
    "    lags = [28, 35, 42, 49, 56]\n",
    "    for lag in lags:\n",
    "        df[f'lag_{lag}'] = df.groupby('id')['sales'].shift(lag)\n",
    "        \n",
    "    windows = [7, 14, 28, 60]\n",
    "    for win in windows:\n",
    "        df[f'rolling_mean_{win}'] = df.groupby('id')['lag_28'].transform(\n",
    "            lambda x: x.rolling(win).mean())\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T19:44:39.486392Z",
     "iopub.status.busy": "2025-11-23T19:44:39.486026Z",
     "iopub.status.idle": "2025-11-23T19:44:39.493118Z",
     "shell.execute_reply": "2025-11-23T19:44:39.492173Z",
     "shell.execute_reply.started": "2025-11-23T19:44:39.486367Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def encode_categoricals(df):\n",
    "    # Added event_name_2 and event_type_2 to the list\n",
    "    cat_cols = [\n",
    "        'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', \n",
    "        'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2'\n",
    "    ]\n",
    "    \n",
    "    # Fill NaNs for ALL event columns\n",
    "    df['event_name_1'] = df['event_name_1'].fillna('NoEvent')\n",
    "    df['event_type_1'] = df['event_type_1'].fillna('NoEvent')\n",
    "    df['event_name_2'] = df['event_name_2'].fillna('NoEvent')\n",
    "    df['event_type_2'] = df['event_type_2'].fillna('NoEvent')\n",
    "    \n",
    "    encoder = LabelEncoder()\n",
    "    for col in cat_cols:\n",
    "        # We convert to string first to handle any mixed types safely\n",
    "        df[col] = encoder.fit_transform(df[col].astype(str))\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T19:44:41.730409Z",
     "iopub.status.busy": "2025-11-23T19:44:41.7299Z",
     "iopub.status.idle": "2025-11-23T19:49:33.434412Z",
     "shell.execute_reply": "2025-11-23T19:49:33.433149Z",
     "shell.execute_reply.started": "2025-11-23T19:44:41.730379Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def transform_and_merge(sales, calendar, prices, config):\n",
    "    # --------------------------------------------------\n",
    "    # 1. Add future forecast columns\n",
    "    # --------------------------------------------------\n",
    "    for day in range(config.FORECAST_HORIZON):\n",
    "        sales[f'd_{config.TRAIN_END + day + 1}'] = np.nan\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 2. Limit history to last ~1000 days (MEMORY FIX)\n",
    "    # --------------------------------------------------\n",
    "    start_idx = max(1, config.TRAIN_END - 1000)\n",
    "\n",
    "    value_cols = [\n",
    "        c for c in sales.columns\n",
    "        if c.startswith('d_') and int(c.split('_')[1]) >= start_idx\n",
    "    ]\n",
    "\n",
    "    id_cols = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 3. Melt after limiting columns\n",
    "    # --------------------------------------------------\n",
    "    data = pd.melt(\n",
    "        sales,\n",
    "        id_vars=id_cols,\n",
    "        value_vars=value_cols,\n",
    "        var_name='d',\n",
    "        value_name='sales'\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 4. Reduce calendar before merge\n",
    "    # --------------------------------------------------\n",
    "    calendar = calendar[\n",
    "        ['d', 'date', 'wm_yr_wk',\n",
    "         'event_name_1', 'event_type_1',\n",
    "         'event_name_2', 'event_type_2']\n",
    "    ]\n",
    "\n",
    "    data = data.merge(calendar, on='d', how='left')\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 5. Reduce prices before merge (CRITICAL)\n",
    "    # --------------------------------------------------\n",
    "    prices = prices[['store_id', 'item_id', 'wm_yr_wk', 'sell_price']]\n",
    "\n",
    "    data = data.merge(\n",
    "        prices,\n",
    "        on=['store_id', 'item_id', 'wm_yr_wk'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 6. Cleanup\n",
    "    # --------------------------------------------------\n",
    "    del calendar, prices\n",
    "    gc.collect()\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T19:49:33.436449Z",
     "iopub.status.busy": "2025-11-23T19:49:33.436112Z",
     "iopub.status.idle": "2025-11-23T19:49:33.443233Z",
     "shell.execute_reply": "2025-11-23T19:49:33.442062Z",
     "shell.execute_reply.started": "2025-11-23T19:49:33.436413Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def perform_split(df, config):\n",
    "    # --------------------------------------------------\n",
    "    # 1. Create masks (lightweight)\n",
    "    # --------------------------------------------------\n",
    "    train_mask = df['d_num'] <= (config.TRAIN_END - config.FORECAST_HORIZON)\n",
    "    valid_mask = (\n",
    "        (df['d_num'] > (config.TRAIN_END - config.FORECAST_HORIZON)) &\n",
    "        (df['d_num'] <= config.TRAIN_END)\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 2. Define feature columns FIRST (CRITICAL)\n",
    "    # --------------------------------------------------\n",
    "    drop_cols = ['id', 'sales', 'wm_yr_wk', 'd_num']\n",
    "    feat_cols = [c for c in df.columns if c not in drop_cols]\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 3. Select only needed columns BEFORE slicing\n",
    "    # --------------------------------------------------\n",
    "    df_feat = df[feat_cols + ['sales']]\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 4. Split (much smaller memory footprint)\n",
    "    # --------------------------------------------------\n",
    "    X_tr = df_feat.loc[train_mask, feat_cols]\n",
    "    y_tr = df_feat.loc[train_mask, 'sales']\n",
    "\n",
    "    X_val = df_feat.loc[valid_mask, feat_cols]\n",
    "    y_val = df_feat.loc[valid_mask, 'sales']\n",
    "\n",
    "    return X_tr, y_tr, X_val, y_val, feat_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "master_df loaded: (31374210, 30)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "master_df = pd.read_pickle(\"processed_dataset.pkl\")\n",
    "print(\"master_df loaded:\", master_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, y_tr, X_val, y_val, feats = perform_split(master_df, ProjectConfig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =========================================================\n",
    "MODEL TRAINING\n",
    "# ========================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T20:02:20.271376Z",
     "iopub.status.busy": "2025-11-23T20:02:20.271026Z",
     "iopub.status.idle": "2025-11-23T20:02:20.277181Z",
     "shell.execute_reply": "2025-11-23T20:02:20.276162Z",
     "shell.execute_reply.started": "2025-11-23T20:02:20.271344Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def run_lgbm(X_tr, y_tr, X_val, y_val, config):\n",
    "    dtrain = lgb.Dataset(X_tr, y_tr)\n",
    "    dvalid = lgb.Dataset(X_val, y_val)\n",
    "    \n",
    "    model = lgb.train(\n",
    "        config.LGB_PARAMS,\n",
    "        dtrain,\n",
    "        valid_sets=[dtrain, dvalid],\n",
    "        valid_names=['train', 'valid'],\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(5),\n",
    "            lgb.log_evaluation(100)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T20:02:20.590332Z",
     "iopub.status.busy": "2025-11-23T20:02:20.589391Z",
     "iopub.status.idle": "2025-11-23T20:23:45.350141Z",
     "shell.execute_reply": "2025-11-23T20:23:45.349055Z",
     "shell.execute_reply.started": "2025-11-23T20:02:20.590303Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 5 rounds\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_valid, y_valid, features = perform_split(master_df, ProjectConfig)\n",
    "\n",
    "model = run_lgbm(X_train, y_train, X_valid, y_valid, ProjectConfig)\n",
    "\n",
    "model.save_model('lgbm_model_v1.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T20:24:04.980597Z",
     "iopub.status.busy": "2025-11-23T20:24:04.980237Z",
     "iopub.status.idle": "2025-11-23T20:24:04.986932Z",
     "shell.execute_reply": "2025-11-23T20:24:04.985741Z",
     "shell.execute_reply.started": "2025-11-23T20:24:04.980574Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def predict_forecast(df, model, config, feats):\n",
    "    pred_mask = df['d_num'] > config.TRAIN_END\n",
    "    X_test = df[pred_mask][feats]\n",
    "    \n",
    "    preds = model.predict(X_test)\n",
    "    \n",
    "    output = df[pred_mask][['id', 'd_num']].copy()\n",
    "    output['sales'] = preds\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T20:24:05.250544Z",
     "iopub.status.busy": "2025-11-23T20:24:05.250204Z",
     "iopub.status.idle": "2025-11-23T20:24:05.256806Z",
     "shell.execute_reply": "2025-11-23T20:24:05.255813Z",
     "shell.execute_reply.started": "2025-11-23T20:24:05.25052Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def format_submission(output_df, config):\n",
    "    output_df = output_df.pivot(index='id', columns='d_num', values='sales').reset_index()\n",
    "    \n",
    "    output_df.columns = ['id'] + [f'F{i}' for i in range(1, config.FORECAST_HORIZON + 1)]\n",
    "    \n",
    "    eval_output = output_df.copy()\n",
    "    eval_output['id'] = eval_output['id'].str.replace('_validation', '_evaluation')\n",
    "    \n",
    "    final_sub = pd.concat([output_df, eval_output], axis=0, sort=False)\n",
    "    \n",
    "    return final_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T20:24:05.444702Z",
     "iopub.status.busy": "2025-11-23T20:24:05.444303Z",
     "iopub.status.idle": "2025-11-23T20:24:47.192501Z",
     "shell.execute_reply": "2025-11-23T20:24:47.191581Z",
     "shell.execute_reply.started": "2025-11-23T20:24:05.444675Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "forecast_df = predict_forecast(master_df, model, ProjectConfig, features)\n",
    "\n",
    "submission = format_submission(forecast_df, ProjectConfig)\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"Submission file generated successfully.\")\n",
    "print(submission.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =========================================================\n",
    "# ML Model Evaluation on Validation Data\n",
    "# ========================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# LightGBM – 28 Day Evaluation\n",
    "# =========================================================\n",
    "\n",
    "# Predict on validation set\n",
    "y_ml_pred = model.predict(X_valid)\n",
    "\n",
    "ml_mae_28 = mean_absolute_error(y_valid, y_ml_pred)\n",
    "ml_rmse_28 = np.sqrt(mean_squared_error(y_valid, y_ml_pred))\n",
    "\n",
    "print(\"LightGBM MAE (28 days):\", ml_mae_28)\n",
    "print(\"LightGBM RMSE (28 days):\", ml_rmse_28)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =========================================================\n",
    "# Baseline Model Comparison \n",
    "# ========================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# =========================================================\n",
    "# Full 28-Day Validation Window\n",
    "# =========================================================\n",
    "\n",
    "valid_mask = (\n",
    "    (master_df[\"d_num\"] > (ProjectConfig.TRAIN_END - ProjectConfig.FORECAST_HORIZON)) &\n",
    "    (master_df[\"d_num\"] <= ProjectConfig.TRAIN_END)\n",
    ")\n",
    "\n",
    "val_df = master_df[valid_mask][[\"id\", \"d_num\", \"sales\"]].copy()\n",
    "\n",
    "print(val_df.head())\n",
    "print(\"Validation days:\", val_df[\"d_num\"].nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# Baseline Model Comparison – Naive Baseline\n",
    "# =========================================================\n",
    "\n",
    "last_sales = (\n",
    "    master_df[master_df[\"d_num\"] <= (ProjectConfig.TRAIN_END - ProjectConfig.FORECAST_HORIZON)]\n",
    "    .groupby(\"id\")[\"sales\"]\n",
    "    .last()\n",
    ")\n",
    "\n",
    "# Create 28-day naive forecast\n",
    "naive_forecast = val_df.copy()\n",
    "naive_forecast[\"pred\"] = naive_forecast[\"id\"].map(last_sales)\n",
    "\n",
    "# Metrics\n",
    "naive_mae_28 = mean_absolute_error(naive_forecast[\"sales\"], naive_forecast[\"pred\"])\n",
    "naive_rmse_28 = np.sqrt(mean_squared_error(naive_forecast[\"sales\"], naive_forecast[\"pred\"]))\n",
    "\n",
    "print(\"Naive MAE (28 days):\", naive_mae_28)\n",
    "print(\"Naive RMSE (28 days):\", naive_rmse_28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# Moving Average (28 days) – 28 Day Evaluation\n",
    "# =========================================================\n",
    "\n",
    "# Average of last 28 training days per item\n",
    "train_data = master_df[master_df[\"d_num\"] <= (ProjectConfig.TRAIN_END - ProjectConfig.FORECAST_HORIZON)]\n",
    "\n",
    "ma_28 = (\n",
    "    train_data\n",
    "    .sort_values(\"d_num\")\n",
    "    .groupby(\"id\")\n",
    "    .tail(28)\n",
    "    .groupby(\"id\")[\"sales\"]\n",
    "    .mean()\n",
    ")\n",
    "\n",
    "# Create forecast\n",
    "ma_forecast = val_df.copy()\n",
    "ma_forecast[\"pred\"] = ma_forecast[\"id\"].map(ma_28)\n",
    "\n",
    "# Metrics\n",
    "ma_mae_28 = mean_absolute_error(ma_forecast[\"sales\"], ma_forecast[\"pred\"])\n",
    "ma_rmse_28 = np.sqrt(mean_squared_error(ma_forecast[\"sales\"], ma_forecast[\"pred\"]))\n",
    "\n",
    "print(\"Moving Avg MAE (28 days):\", ma_mae_28)\n",
    "print(\"Moving Avg RMSE (28 days):\", ma_rmse_28)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =========================================================\n",
    "# Final Model Comparison (Baseline vs ML)\n",
    "# ========================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# Final 28-Day Model Comparison\n",
    "# =========================================================\n",
    "\n",
    "comparison_28_df = pd.DataFrame({\n",
    "    \"Model\": [\n",
    "        \"Naive Baseline\",\n",
    "        \"Moving Average (28 days)\",\n",
    "        \"LightGBM (Tweedie)\"\n",
    "    ],\n",
    "    \"MAE (28 days)\": [\n",
    "        naive_mae_28,\n",
    "        ma_mae_28,\n",
    "        ml_mae_28\n",
    "    ],\n",
    "    \"RMSE (28 days)\": [\n",
    "        naive_rmse_28,\n",
    "        ma_rmse_28,\n",
    "        ml_rmse_28\n",
    "    ]\n",
    "})\n",
    "\n",
    "comparison_28_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_improvement = (ma_rmse_28 - ml_rmse_28) / ma_rmse_28 * 100\n",
    "mae_improvement = (ma_mae_28 - ml_mae_28) / ma_mae_28 * 100\n",
    "\n",
    "print(f\"RMSE Improvement over MA: {rmse_improvement:.2f}%\")\n",
    "print(f\"MAE Improvement over MA: {mae_improvement:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T20:27:26.848598Z",
     "iopub.status.busy": "2025-11-23T20:27:26.848161Z",
     "iopub.status.idle": "2025-11-23T20:27:27.594818Z",
     "shell.execute_reply": "2025-11-23T20:27:27.593989Z",
     "shell.execute_reply.started": "2025-11-23T20:27:26.848566Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "def plot_item_forecast(master_df, forecast_df, item_id, config):\n",
    "    history_mask = (master_df['id'] == item_id) & (master_df['d_num'] <= config.TRAIN_END)\n",
    "    # Show last 90 days of history for clarity\n",
    "    history_mask = history_mask & (master_df['d_num'] > config.TRAIN_END - 90)\n",
    "    history = master_df[history_mask]\n",
    "    \n",
    "    forecast = forecast_df[forecast_df['id'] == item_id]\n",
    "    \n",
    "    plt.figure(figsize=(14, 7))\n",
    "    \n",
    "    plt.plot(history['d_num'], history['sales'], label='Historical Sales (Last 90 Days)', color='#1f77b4', linewidth=2)\n",
    "    \n",
    "    plt.plot(forecast['d_num'], forecast['sales'], label='Forecast (Next 28 Days)', color='#ff7f0e', linestyle='--', linewidth=2.5)\n",
    "    \n",
    "    plt.axvline(x=config.TRAIN_END, color='grey', linestyle=':', linewidth=1.5, label='Forecast Start')\n",
    "    \n",
    "    plt.title(f'Sales Forecast for Item: {item_id}', fontsize=16)\n",
    "    plt.xlabel('Day Number', fontsize=12)\n",
    "    plt.ylabel('Sales Volume', fontsize=12)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def visualize_results(master_df, forecast_df, config, num_items=3):\n",
    "    unique_ids = forecast_df['id'].unique()\n",
    "    random_items = np.random.choice(unique_ids, num_items, replace=False)\n",
    "    \n",
    "    for item_id in random_items:\n",
    "        plot_item_forecast(master_df, forecast_df, item_id, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T20:27:33.455018Z",
     "iopub.status.busy": "2025-11-23T20:27:33.454373Z",
     "iopub.status.idle": "2025-11-23T20:27:42.370492Z",
     "shell.execute_reply": "2025-11-23T20:27:42.369554Z",
     "shell.execute_reply.started": "2025-11-23T20:27:33.45499Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Ensure you have run all previous sections first\n",
    "visualize_results(master_df, forecast_df, ProjectConfig, num_items=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "item_meta = sales[['id','item_id','dept_id','cat_id','store_id','state_id']]\n",
    "item_meta = item_meta.drop_duplicates()\n",
    "item_meta.to_csv(\"item_master.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "DATA_PATH = \"D:/M5 Data\"\n",
    "\n",
    "sales = pd.read_csv(\n",
    "    f\"{DATA_PATH}/sales_train_evaluation.csv\"\n",
    ")\n",
    "\n",
    "calendar = pd.read_csv(\n",
    "    f\"{DATA_PATH}/calendar.csv\",\n",
    "    usecols=[\"d\", \"date\"]\n",
    ")\n",
    "\n",
    "print(\"Sales shape:\", sales.shape)\n",
    "print(\"Calendar shape:\", calendar.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "id_cols = [\"id\"]\n",
    "\n",
    "sales_long = sales.melt(\n",
    "    id_vars=id_cols,\n",
    "    var_name=\"d\",\n",
    "    value_name=\"sales\"\n",
    ")\n",
    "\n",
    "print(sales_long.head())\n",
    "print(\"Long format shape:\", sales_long.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sales_long = sales_long.merge(calendar, on=\"d\", how=\"left\")\n",
    "\n",
    "sales_long[\"date\"] = pd.to_datetime(sales_long[\"date\"])\n",
    "\n",
    "print(sales_long.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "last_date = sales_long[\"date\"].max()\n",
    "cutoff_date = last_date - pd.Timedelta(days=90)\n",
    "\n",
    "historical_90_days = sales_long[\n",
    "    sales_long[\"date\"] >= cutoff_date\n",
    "][[\"id\", \"date\", \"sales\"]]\n",
    "\n",
    "historical_90_days.to_csv(\n",
    "    \"historical_90_days.csv\",\n",
    "    index=False\n",
    ")\n",
    "\n",
    "print(\"historical_90_days.csv saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sales_long[\"year\"] = sales_long[\"date\"].dt.year\n",
    "sales_long[\"month\"] = sales_long[\"date\"].dt.month\n",
    "\n",
    "historical_monthly = (\n",
    "    sales_long\n",
    "    .groupby([\"id\", \"year\", \"month\"], as_index=False)\n",
    "    [\"sales\"]\n",
    "    .sum()\n",
    ")\n",
    "\n",
    "historical_monthly.to_csv(\n",
    "    \"historical_monthly.csv\",\n",
    "    index=False\n",
    ")\n",
    "\n",
    "print(\"historical_monthly.csv saved\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 1236839,
     "sourceId": 18599,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
